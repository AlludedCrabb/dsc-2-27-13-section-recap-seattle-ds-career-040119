
# Section Recap

## Introduction

This short lesson summarizes the topics we covered in section 27 and why they'll be important to you as a data scientist.

## Objectives
You will be able to:
* Understand and explain what was covered in this section
* Understand and explain why this section will help you become a data scientist

## Key Takeaways

The key takeaways from this section include:
* Manhattan and Euclidian are simply special cases of Minkowski distance whith values of 1 and 2 respectively
* kNN can be used for regression by calculating the mean of the nearest points
* kNN can be used as a classifier by calculating the mode of the nearest points 
* A confusion matrix is a powerful tool for evaluating model fit - especially when the cost of false positives and false negatives are different
* $$Precision = \frac{\text{Number of True Positives}}{\text{Number of Predicted Positives}}$$
* $$Recall = \frac{\text{Number of True Positives}}{\text{Number of Actual Total Positives}}$$
* $$Accuracy = \frac{\text{Number of True Positives + True Negatives}}{\text{Total Observations}}$$
* $$F1-Score = 2\ \frac{Precision\ x\ Recall}{Precision + Recall}$$
* When optimizing k, too low will generally overfit and too high will generally underfit
* Plot error for a range of plausible k's to find the best k for a given data set

